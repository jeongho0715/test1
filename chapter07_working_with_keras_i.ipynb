{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aGeVaN8Rzgt"
      },
      "source": [
        "**The `Sequential` class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PATH\"] += os.pathsep + \"C:/Program Files/Graphviz/bin\"\n",
        "\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l8Iw7xOFRzgt"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "]) # can make model using list of layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tulRoovjRzgv"
      },
      "source": [
        "**Incrementally building a Sequential model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uytDsCPcRzgv"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential() # can make this way, add layer one by one\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# can check summary, structure, but we can see output shape or param. in this model, we dont' know input shape, didnt selected.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:3214\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \n\u001b[0;32m   3186\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   3212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 3214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3218\u001b[0m     )\n\u001b[0;32m   3219\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[0;32m   3220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3221\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3226\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[0;32m   3227\u001b[0m )\n",
            "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ],
      "source": [
        "model.summary() # can check summary, structure, but we can see output shape or param. in this model, we dont' know input shape, didnt selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq3zUCfSRzgw"
      },
      "source": [
        "**Calling a model for the first time to build it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lqKKYghIRzgw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_2/kernel:0' shape=(3, 64) dtype=float32, numpy=\n",
              " array([[-0.07877834,  0.08633274, -0.15067293, -0.00724667,  0.17307723,\n",
              "         -0.26464337,  0.26820278,  0.2438345 , -0.0758999 , -0.00764075,\n",
              "         -0.16053292,  0.2094832 , -0.2981741 ,  0.1965091 ,  0.03571182,\n",
              "          0.10837075,  0.17896953,  0.17407525,  0.22734475,  0.16140622,\n",
              "          0.24995267,  0.05086115, -0.13049124,  0.14878544, -0.02756557,\n",
              "          0.08818391,  0.2081784 , -0.20035228, -0.29449907, -0.21188688,\n",
              "         -0.11416866,  0.21374208, -0.0686821 , -0.1994389 ,  0.15552491,\n",
              "          0.05259803,  0.04914418,  0.01599723, -0.24216494,  0.15842333,\n",
              "         -0.02838972, -0.20922491,  0.06809399, -0.24359731, -0.04109299,\n",
              "         -0.26844177, -0.10241809, -0.01901874,  0.18435311,  0.01804605,\n",
              "         -0.15456693, -0.19636546, -0.17429093,  0.17540222,  0.11036143,\n",
              "         -0.25507554,  0.23925078,  0.22059876,  0.22483474,  0.11313397,\n",
              "         -0.21854267, -0.10156392, -0.1480315 ,  0.22797781],\n",
              "        [ 0.2133885 , -0.08851555,  0.10132292, -0.29832274,  0.09124115,\n",
              "         -0.285023  ,  0.04522178,  0.02143392, -0.17922474, -0.18097982,\n",
              "         -0.11708534, -0.10318144,  0.29790765,  0.23033172,  0.1331084 ,\n",
              "         -0.00840759, -0.28615963,  0.04765645,  0.08275709, -0.20146587,\n",
              "         -0.14819275,  0.26372784,  0.27362597, -0.2886612 ,  0.14741963,\n",
              "         -0.28552192,  0.13056901,  0.19061822, -0.08760273, -0.1066993 ,\n",
              "          0.02201203, -0.11445555, -0.21218511, -0.06974925, -0.20654196,\n",
              "         -0.21527547,  0.02452204,  0.2706927 ,  0.14040282, -0.28920874,\n",
              "          0.24980754,  0.07228744,  0.06870243, -0.1753032 ,  0.05755532,\n",
              "         -0.1790732 , -0.10239719,  0.28554004, -0.16189781, -0.15708892,\n",
              "         -0.00517398,  0.18756005, -0.15677492,  0.14610761,  0.00774691,\n",
              "         -0.12922168, -0.27426973, -0.02515796, -0.18173788, -0.07134444,\n",
              "          0.04447192, -0.21484901, -0.07448322,  0.19336873],\n",
              "        [-0.13246486,  0.07062325,  0.28602248,  0.22742134,  0.26045   ,\n",
              "          0.17395967,  0.12023363,  0.29382378,  0.24886268,  0.0522117 ,\n",
              "          0.17188808, -0.26404476, -0.24514142,  0.27082437, -0.20971678,\n",
              "          0.04230917, -0.17817423, -0.02570349,  0.01639527,  0.2632919 ,\n",
              "          0.22384226,  0.19524083,  0.25793737, -0.1499902 , -0.076315  ,\n",
              "         -0.10628413,  0.22509193, -0.10284489,  0.0385254 , -0.03040519,\n",
              "          0.18547663,  0.21959049, -0.21763256, -0.17631733,  0.11422575,\n",
              "          0.02622646,  0.2177304 ,  0.2991438 , -0.1996881 ,  0.11716098,\n",
              "          0.09316406,  0.2682472 , -0.15071303,  0.16692814, -0.12769021,\n",
              "         -0.26945257, -0.21962073, -0.23863995, -0.00227755, -0.26829493,\n",
              "         -0.02457991,  0.28023368, -0.0162431 , -0.22754194,  0.27226537,\n",
              "         -0.25898704,  0.11571389, -0.23084432,  0.23270583,  0.132822  ,\n",
              "         -0.12216613,  0.20362294,  0.00858402, -0.15459318]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_3/kernel:0' shape=(64, 10) dtype=float32, numpy=\n",
              " array([[-0.14415741,  0.21552232, -0.23683298,  0.20215356, -0.04060975,\n",
              "          0.10180387,  0.13399252, -0.10389221, -0.28356096, -0.04711738],\n",
              "        [-0.08922586,  0.06818691,  0.11329517,  0.2782168 , -0.26207325,\n",
              "         -0.07923815, -0.17225276,  0.09239614, -0.11915387, -0.10790206],\n",
              "        [ 0.00131351,  0.1573042 , -0.01764151,  0.25640723, -0.05806749,\n",
              "         -0.23184103, -0.11921728, -0.01139641, -0.08975539, -0.1329471 ],\n",
              "        [-0.21017817, -0.22730123, -0.15372413, -0.18179524,  0.03423008,\n",
              "         -0.24799326, -0.22744949, -0.16530126,  0.08462033, -0.0315378 ],\n",
              "        [ 0.13778731, -0.15666415,  0.13947856,  0.04182509, -0.02645305,\n",
              "          0.04822737, -0.12405084,  0.25038287,  0.25886253,  0.20621136],\n",
              "        [-0.09273838,  0.25844488, -0.22337703, -0.18831192,  0.01931009,\n",
              "         -0.19621645, -0.03721556, -0.1212858 , -0.23990293, -0.04173508],\n",
              "        [ 0.01733553, -0.11135755,  0.26426062, -0.1520749 ,  0.12182048,\n",
              "         -0.05341749, -0.21302849, -0.06745484,  0.07956845, -0.06505999],\n",
              "        [-0.24136478, -0.08401258, -0.23848669,  0.09249622,  0.13583937,\n",
              "          0.15624568, -0.08867711, -0.1936188 ,  0.05066451, -0.06655824],\n",
              "        [ 0.16745028,  0.01433253, -0.24104612, -0.14399394,  0.04071987,\n",
              "          0.264034  , -0.19919047,  0.26803938, -0.00061697,  0.10398757],\n",
              "        [-0.2167499 ,  0.22677407, -0.19973066,  0.15477493, -0.00708613,\n",
              "         -0.22617094,  0.0340423 , -0.08247714,  0.03493869, -0.26958445],\n",
              "        [ 0.21625146, -0.20297325, -0.13594413,  0.08611193, -0.251522  ,\n",
              "          0.04801074,  0.24599746,  0.09816346,  0.13222557,  0.16042131],\n",
              "        [ 0.25416765,  0.06796667, -0.08837569,  0.23359838,  0.13300046,\n",
              "         -0.20772405, -0.15794548,  0.01691842, -0.08274625,  0.07124239],\n",
              "        [-0.23559427,  0.02898076, -0.1234909 ,  0.11044532, -0.03683898,\n",
              "          0.03022626,  0.11451256, -0.01961607, -0.07339942,  0.1995534 ],\n",
              "        [-0.2527113 ,  0.24891159,  0.20920518, -0.09909597,  0.14529124,\n",
              "          0.25320724, -0.25080842, -0.04014926,  0.02486357, -0.19293809],\n",
              "        [-0.20614007, -0.22611453,  0.12169379, -0.26208955, -0.2328557 ,\n",
              "          0.27076504,  0.1412316 ,  0.03988734,  0.10128534,  0.24301955],\n",
              "        [ 0.03139278,  0.07994303, -0.16245162,  0.05221301, -0.24001414,\n",
              "         -0.09265962,  0.16495124,  0.24883291, -0.150066  ,  0.06562665],\n",
              "        [-0.2160078 ,  0.0113087 , -0.06809111,  0.1818125 , -0.00589895,\n",
              "          0.00814071, -0.20806716, -0.21966845,  0.16374871,  0.13499415],\n",
              "        [ 0.13511378,  0.18002999, -0.23443474, -0.0941844 ,  0.19638944,\n",
              "          0.11845461,  0.06099501,  0.12237269,  0.08574587,  0.17714188],\n",
              "        [-0.1185282 ,  0.0237011 ,  0.27507403, -0.13688399,  0.16719198,\n",
              "          0.28039226, -0.22050539, -0.25329384,  0.23513272,  0.22094598],\n",
              "        [-0.22856654, -0.260259  , -0.15975948, -0.24601288,  0.12244987,\n",
              "          0.11468855,  0.06175935,  0.15197992, -0.25164604, -0.06367153],\n",
              "        [-0.25069436, -0.02670273, -0.07308951, -0.11473714,  0.12700552,\n",
              "          0.14550963,  0.21598926, -0.00658932,  0.25349   ,  0.05608642],\n",
              "        [-0.07499501,  0.2614666 ,  0.07455909, -0.23767366, -0.2009528 ,\n",
              "         -0.20724048, -0.17457858, -0.21002868,  0.11263633,  0.09054303],\n",
              "        [-0.18157291, -0.15200722,  0.09694281, -0.2531664 ,  0.11224955,\n",
              "          0.06106764, -0.2510996 , -0.04770111, -0.25464505, -0.0250707 ],\n",
              "        [-0.0254364 , -0.2795665 ,  0.08887997,  0.14060077,  0.1806803 ,\n",
              "         -0.18322839,  0.15259677, -0.19251744,  0.12100941,  0.2153596 ],\n",
              "        [-0.10283218,  0.04500645,  0.21072325, -0.15414669,  0.14158294,\n",
              "          0.18906596,  0.21139657,  0.08213416,  0.0365077 ,  0.24337968],\n",
              "        [-0.24478796, -0.04238626,  0.20020664,  0.18072748, -0.23950584,\n",
              "          0.07504153, -0.1402474 , -0.26573303,  0.06129134,  0.12319258],\n",
              "        [ 0.07722715, -0.22145164, -0.28366417, -0.17183775,  0.02462199,\n",
              "          0.13440603, -0.03531434, -0.01334551, -0.06854977,  0.18203253],\n",
              "        [ 0.06104028, -0.0738536 , -0.12317847,  0.06443501,  0.2244015 ,\n",
              "         -0.26416647,  0.0897896 ,  0.00734925, -0.10304175, -0.19920582],\n",
              "        [-0.1757904 , -0.24059655, -0.06794147,  0.27819046,  0.19206366,\n",
              "          0.07896897, -0.0402483 ,  0.26127252, -0.26823494, -0.15911861],\n",
              "        [ 0.06313324,  0.00339684,  0.044945  ,  0.04284242, -0.00853664,\n",
              "         -0.14622627,  0.12383619, -0.17574579, -0.15214   , -0.18625352],\n",
              "        [ 0.02750328,  0.1979819 , -0.16260579, -0.04585506, -0.12047872,\n",
              "          0.05464533, -0.00160822,  0.17936829,  0.07095024,  0.19642425],\n",
              "        [-0.12994531, -0.0600193 ,  0.07806632,  0.21499544,  0.09237435,\n",
              "          0.01735863,  0.1122953 ,  0.21495566,  0.12015048, -0.10553525],\n",
              "        [-0.22305469, -0.11053514,  0.255596  ,  0.04290995, -0.27255303,\n",
              "         -0.04715765, -0.09452873, -0.11029734, -0.02188283,  0.26354167],\n",
              "        [ 0.2114591 ,  0.06471702, -0.27322185,  0.01471245, -0.0868164 ,\n",
              "         -0.17472358,  0.228717  , -0.2697926 ,  0.2409893 , -0.08337538],\n",
              "        [-0.18898675, -0.2699099 , -0.14215101, -0.01204848, -0.08194503,\n",
              "         -0.17036124, -0.19276538, -0.15405224,  0.24765381, -0.17478225],\n",
              "        [-0.09505562, -0.01377571, -0.09255391, -0.0864149 ,  0.07802701,\n",
              "          0.11912039,  0.06515729,  0.14314103,  0.22376332,  0.22899476],\n",
              "        [-0.01476574, -0.28342083, -0.03117546,  0.13455701, -0.2622468 ,\n",
              "          0.064897  ,  0.02922693, -0.09025247, -0.25821853,  0.15986177],\n",
              "        [-0.22450854,  0.03901991, -0.06081156, -0.1399504 ,  0.21973965,\n",
              "          0.21602097, -0.03706634,  0.23774174,  0.16572449, -0.04697442],\n",
              "        [-0.10520327, -0.07560174,  0.05467793, -0.2501453 ,  0.18282416,\n",
              "          0.20607856,  0.16133457, -0.08597289, -0.19141507,  0.16599467],\n",
              "        [-0.15934406,  0.01243007,  0.23056498, -0.03742175,  0.02630302,\n",
              "         -0.2513613 , -0.08242568, -0.16078982,  0.04018971,  0.17245024],\n",
              "        [ 0.16615862,  0.20364413,  0.25905713, -0.08062288,  0.15343052,\n",
              "         -0.13278165, -0.2045106 ,  0.18076545,  0.17977637, -0.03299591],\n",
              "        [ 0.03763077, -0.22608629, -0.0467869 , -0.1087798 ,  0.23536637,\n",
              "         -0.24312142, -0.22063377,  0.02429453,  0.18169925, -0.22465375],\n",
              "        [-0.09480484,  0.22524187,  0.16688722,  0.14006084,  0.22841284,\n",
              "          0.22299948, -0.24961106,  0.18492255,  0.06992927, -0.28030276],\n",
              "        [ 0.23339936,  0.21226645, -0.13902527, -0.1367518 , -0.1537496 ,\n",
              "          0.01438704,  0.13119856, -0.18297571,  0.24703553, -0.27023467],\n",
              "        [ 0.20700544,  0.09791517,  0.18343884,  0.24886134,  0.27041206,\n",
              "         -0.13635886, -0.27021682, -0.17220151, -0.2736366 , -0.10923263],\n",
              "        [-0.04255375,  0.272926  , -0.021227  , -0.01059544,  0.20604089,\n",
              "         -0.17330585,  0.25720552, -0.19492322,  0.1615256 , -0.16559522],\n",
              "        [ 0.08838445, -0.25610614,  0.2665876 , -0.17317095, -0.16608423,\n",
              "          0.10077018, -0.12955175, -0.2656219 ,  0.2535145 , -0.17328352],\n",
              "        [ 0.00972295, -0.04280412, -0.18177754,  0.1608654 ,  0.15032494,\n",
              "         -0.08126701, -0.24024713,  0.04765806, -0.23157483, -0.26109225],\n",
              "        [-0.10798687,  0.2674198 ,  0.02560017,  0.17736745, -0.15755437,\n",
              "         -0.07500066,  0.21314085,  0.18785983, -0.15044157, -0.14253615],\n",
              "        [-0.24055365, -0.1227175 , -0.247271  , -0.03747925, -0.09115098,\n",
              "         -0.0136072 , -0.01702026,  0.19954953,  0.02086103,  0.03465316],\n",
              "        [ 0.07192996, -0.21841298,  0.12862712,  0.01251501,  0.14224139,\n",
              "          0.06890041, -0.26578835, -0.24400085, -0.04641107,  0.2688813 ],\n",
              "        [ 0.18112254, -0.20445682, -0.01570877,  0.18942708, -0.18739548,\n",
              "         -0.03149956, -0.05579701, -0.1759401 , -0.10374379, -0.08944629],\n",
              "        [ 0.00694174,  0.02594626,  0.20622024, -0.17022532,  0.07024184,\n",
              "         -0.15797766,  0.20861259,  0.2139372 , -0.06034605,  0.24597457],\n",
              "        [ 0.26953927, -0.1458086 , -0.23737073,  0.10092306,  0.00207692,\n",
              "          0.07693392, -0.06977509,  0.12001932,  0.26982227,  0.15278515],\n",
              "        [ 0.09238875, -0.13239618, -0.2104775 , -0.19262417, -0.09689596,\n",
              "         -0.2421876 , -0.22291084, -0.16274646,  0.08359018,  0.2469143 ],\n",
              "        [-0.01690105,  0.24976102, -0.14152597,  0.06122303, -0.12304045,\n",
              "         -0.19782963,  0.1078335 , -0.25814342,  0.14268726, -0.28108078],\n",
              "        [ 0.17022648,  0.1360681 , -0.1820871 , -0.24271849,  0.20887437,\n",
              "          0.26391396, -0.22766423, -0.18843296, -0.19314957,  0.02448928],\n",
              "        [ 0.11552235, -0.08258739, -0.24388415,  0.15246609, -0.13389917,\n",
              "          0.16494423, -0.2450067 , -0.01674572,  0.09161997,  0.15950245],\n",
              "        [ 0.07481369, -0.27921233,  0.23237458, -0.21270806, -0.25065744,\n",
              "          0.21205804, -0.07111013,  0.13221884, -0.03577042,  0.1508362 ],\n",
              "        [-0.27517945,  0.06492701,  0.07000253, -0.07558355, -0.23354213,\n",
              "         -0.13248707, -0.06873979, -0.08771764,  0.09161806,  0.20436165],\n",
              "        [-0.07126912,  0.09550253, -0.14371009, -0.20472431, -0.24244863,\n",
              "          0.1350188 , -0.16622801, -0.10253374,  0.03110889,  0.2729462 ],\n",
              "        [ 0.22104463,  0.24591401, -0.18481639,  0.21086624, -0.09711817,\n",
              "         -0.06044815,  0.04425117,  0.1859827 ,  0.08519384, -0.23039751],\n",
              "        [-0.02135336, -0.17540357,  0.18964675, -0.16710249,  0.27021483,\n",
              "         -0.12517977, -0.08170524,  0.11433476, -0.03834021,  0.25749394],\n",
              "        [ 0.2651362 ,  0.08754173, -0.19011018,  0.04788765,  0.05899513,\n",
              "          0.13665295, -0.17294407, -0.2253321 ,  0.13520595,  0.08449268]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.build(input_shape=(None, 3)) # this model, we set input shape\n",
        "model.weights # can check the weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model.weights) # we can see the length of weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([3, 64])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.weights[0].shape # we can see the dimension of weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wgzriyrRzgw"
      },
      "source": [
        "**The summary method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RndPqiS7Rzgw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 64)                256       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 906\n",
            "Trainable params: 906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary() # now we can see output shape, and apram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrOytOr8Rzgx"
      },
      "source": [
        "**Naming models and layers with the `name` argument**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4WeWAsrcRzgx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_example_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " my_first_layer (Dense)      (None, 64)                256       \n",
            "                                                                 \n",
            " my_last_layer (Dense)       (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 906\n",
            "Trainable params: 906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(name=\"my_example_model\") # set name of layer\n",
        "model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n",
        "model.build((None, 3))\n",
        "model.summary() # on summaery, we can see the name, basically, keras automatically name the layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxZB1M_VRzgx"
      },
      "source": [
        "**Specifying the input shape of your model in advance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mo-zVwN9Rzgx"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(3,))) # another way to set input dimension\n",
        "model.add(layers.Dense(64, activation=\"relu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tWfeUwzsRzgx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 64)                256       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256\n",
            "Trainable params: 256\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary() # can see the output shape and number of param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZWYoLnWTRzgx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 64)                256       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 906\n",
            "Trainable params: 906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.add(layers.Dense(10, activation=\"softmax\")) # can add more layer\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sequential model can make model only sequentially \n",
        "layer들의 결과를 합킨 다음에 쓰거나 앞선 결과물을 나중에 다시 쓰려고 할 때, sqeuentially 할 경우, 이전 결과가 저장되지 않아 다시 사용할 수 없음. \n",
        "Funcitonal API를 통해서 layer를 일종의 function으로 사용함으로써 그 결과는 변수에 저장하거나, 이전에 저장한 변수의 결과물을 다시 함수에 넣어서 이전의 결과값을 사용하거나 여러 레이어의 결과를 동시에 만든 다음에 하나로 합쳐서 또 다른 레이어의 input으로 사용하는 등 더 유연한 사용이 가능함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTr1Kq7HRzgx"
      },
      "source": [
        "### The Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTDZQOogRzgy"
      },
      "source": [
        "#### A simple example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-LQEfyoRzgy"
      },
      "source": [
        "**A simple Functional model with two `Dense` layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iwd7afTxRzgy"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
        "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FcIPJt3KRzgy"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"my_input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X2wMXEm0Rzgy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([None, 3])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.shape # check the shape of input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c_Z5MynaRzgy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.dtype # check the type of input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aI5hL-6bRzgy"
      },
      "outputs": [],
      "source": [
        "features = layers.Dense(64, activation=\"relu\")(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q99VzshnRzgy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([None, 64])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CXPq1vhjRzgz"
      },
      "outputs": [],
      "source": [
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lougsSiMRzgz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " my_input (InputLayer)       [(None, 3)]               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                256       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 906\n",
            "Trainable params: 906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary() # same as previous model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7eCbqfDRzgz"
      },
      "source": [
        "#### Multi-input, multi-output models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m076Jd7Rzgz"
      },
      "source": [
        "**A multi-input, multi-output Functional model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "s8WLMpNCRzgz"
      },
      "outputs": [],
      "source": [
        "vocabulary_size = 10000\n",
        "num_tags = 100\n",
        "num_departments = 4\n",
        "\n",
        "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
        "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
        "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
        "\n",
        "features = layers.Concatenate()([title, text_body, tags])\n",
        "features = layers.Dense(64, activation=\"relu\")(features)\n",
        "\n",
        "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
        "department = layers.Dense(\n",
        "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
        "\n",
        "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLngnB5qRzgz"
      },
      "source": [
        "#### Training a multi-input, multi-output model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ONLTuKRzgz"
      },
      "source": [
        "**Training a model by providing lists of input & target arrays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lUacJtTDRzgz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 4s 35ms/step - loss: 30.9259 - priority_loss: 0.3215 - department_loss: 30.6044 - priority_mean_absolute_error: 0.4904 - department_accuracy: 0.2414\n",
            "40/40 [==============================] - 1s 13ms/step - loss: 31.8595 - priority_loss: 0.3247 - department_loss: 31.5348 - priority_mean_absolute_error: 0.4930 - department_accuracy: 0.2500\n",
            "40/40 [==============================] - 0s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = 1280\n",
        "\n",
        "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
        "\n",
        "priority_data = np.random.random(size=(num_samples, 1))\n",
        "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
        "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]]) # we have two output, we nee two loss, \n",
        "model.fit([title_data, text_body_data, tags_data],\n",
        "          [priority_data, department_data],\n",
        "          epochs=1)\n",
        "model.evaluate([title_data, text_body_data, tags_data],\n",
        "               [priority_data, department_data])\n",
        "priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtD1DEGURzgz"
      },
      "source": [
        "**Training a model by providing dicts of input & target arrays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-HGih3HARzgz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 2s 33ms/step - loss: 40.6864 - priority_loss: 0.3247 - department_loss: 40.3617 - priority_mean_absolute_error: 0.4930 - department_accuracy: 0.2797\n",
            "40/40 [==============================] - 1s 15ms/step - loss: 28.6983 - priority_loss: 0.3247 - department_loss: 28.3736 - priority_mean_absolute_error: 0.4930 - department_accuracy: 0.5844\n",
            "40/40 [==============================] - 0s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
        "              metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\n",
        "model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "          {\"priority\": priority_data, \"department\": department_data},\n",
        "          epochs=1)\n",
        "model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "               {\"priority\": priority_data, \"department\": department_data})\n",
        "priority_preds, department_preds = model.predict(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}) # with dictionary structured data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkxG0ynLRzg0"
      },
      "source": [
        "#### The power of the Functional API: Access to layer connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ScVga0qARzg5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "keras.utils.plot_model(model, \"ticket_classifier.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-iWL7llGRzg5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "keras.utils.plot_model(model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocabulary_size = 10000\n",
        "num_tags = 100\n",
        "num_departments = 4\n",
        "\n",
        "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
        "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
        "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
        "\n",
        "features = layers.Concatenate()([title, text_body, tags])\n",
        "features = layers.Dense(64, activation=\"relu\")(features)\n",
        "features = layers.Dense(36, activation=\"relu\")(features)\n",
        "\n",
        "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
        "department = layers.Dense(\n",
        "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
        "\n",
        "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocabulary_size = 10000\n",
        "num_tags = 100\n",
        "num_departments = 4\n",
        "\n",
        "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
        "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
        "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
        "\n",
        "features = layers.Concatenate()([title, text_body, tags])\n",
        "features1 = layers.Dense(64, activation=\"relu\")(features)\n",
        "features2 = layers.Dense(36, activation=\"relu\")(features1)\n",
        "\n",
        "# difficulty = (features1)\n",
        "\n",
        "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features2)\n",
        "department = layers.Dense(\n",
        "    num_departments, activation=\"softmax\", name=\"department\")(features2)\n",
        "\n",
        "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p3zDLTERzg6"
      },
      "source": [
        "**Retrieving the inputs or outputs of a layer in a Functional model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w9KuLOmPRzg6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x2444ac37700>,\n",
              " <keras.engine.input_layer.InputLayer at 0x2444ac376a0>,\n",
              " <keras.engine.input_layer.InputLayer at 0x2444ac331f0>,\n",
              " <keras.layers.merging.concatenate.Concatenate at 0x2444ac333a0>,\n",
              " <keras.layers.core.dense.Dense at 0x2444ac338b0>,\n",
              " <keras.layers.core.dense.Dense at 0x2444ac33040>,\n",
              " <keras.layers.core.dense.Dense at 0x24449903a00>,\n",
              " <keras.layers.core.dense.Dense at 0x244499093d0>]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LPyfvKMdRzg6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'title')>,\n",
              " <KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'text_body')>,\n",
              " <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'tags')>]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers[3].input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tVnyFLQwRzg6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer 'concatenate_2')>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers[3].output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2wGUKX7Rzg6"
      },
      "source": [
        "**Creating a new model by reusing intermediate layer outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9e5IPbCjRzg6"
      },
      "outputs": [],
      "source": [
        "features = model.layers[4].output\n",
        "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
        "\n",
        "new_model = keras.Model(\n",
        "    inputs=[title, text_body, tags],\n",
        "    outputs=[priority, department, difficulty])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tyuqcynGRzg6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "keras.utils.plot_model(new_model, \"updated_ticket_classifier.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0aAa8goRzg6"
      },
      "source": [
        "### Subclassing the Model class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVW6vf2TRzg6"
      },
      "source": [
        "#### Rewriting our previous example as a subclassed model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx7k2NNNRzg6"
      },
      "source": [
        "**A simple subclassed model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bA-0WkqVRzg6"
      },
      "outputs": [],
      "source": [
        "class CustomerTicketModel(keras.Model):\n",
        "\n",
        "    def __init__(self, num_departments):\n",
        "        super().__init__()\n",
        "        self.concat_layer = layers.Concatenate()\n",
        "        self.mixing_layer = layers.Dense(64, activation=\"relu\") # intermediate dense layer\n",
        "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
        "        self.department_classifier = layers.Dense(\n",
        "            num_departments, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        title = inputs[\"title\"]\n",
        "        text_body = inputs[\"text_body\"]\n",
        "        tags = inputs[\"tags\"]\n",
        "\n",
        "        features = self.concat_layer([title, text_body, tags])\n",
        "        features = self.mixing_layer(features)\n",
        "        priority = self.priority_scorer(features)\n",
        "        department = self.department_classifier(features)\n",
        "        return priority, department"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eCSggRiDRzg7"
      },
      "outputs": [],
      "source": [
        "model = CustomerTicketModel(num_departments=4)\n",
        "\n",
        "priority, department = model(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "h3zSX9WKRzg7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 3s 41ms/step - loss: 27.2884 - output_1_loss: 0.3248 - output_2_loss: 26.9637 - output_1_mean_absolute_error: 0.4931 - output_2_accuracy: 0.1836\n",
            "40/40 [==============================] - 1s 8ms/step - loss: 11.8740 - output_1_loss: 0.3247 - output_2_loss: 11.5493 - output_1_mean_absolute_error: 0.4930 - output_2_accuracy: 0.1125\n",
            "40/40 [==============================] - 0s 8ms/step\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
        "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
        "model.fit({\"title\": title_data,\n",
        "           \"text_body\": text_body_data,\n",
        "           \"tags\": tags_data},\n",
        "          [priority_data, department_data],\n",
        "          epochs=1)\n",
        "model.evaluate({\"title\": title_data,\n",
        "                \"text_body\": text_body_data,\n",
        "                \"tags\": tags_data},\n",
        "               [priority_data, department_data])\n",
        "priority_preds, department_preds = model.predict({\"title\": title_data,\n",
        "                                                  \"text_body\": text_body_data,\n",
        "                                                  \"tags\": tags_data})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjVb4O88Rzg7"
      },
      "source": [
        "#### Beware: What subclassed models don't support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-vosUjRzg7"
      },
      "source": [
        "### Mixing and matching different components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X51z-4JsRzg7"
      },
      "source": [
        "**Creating a Functional model that includes a subclassed model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Ev1IvqiGRzg7"
      },
      "outputs": [],
      "source": [
        "class Classifier(keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__() # initialization fucntino\n",
        "        if num_classes == 2:\n",
        "            num_units = 1\n",
        "            activation = \"sigmoid\"\n",
        "        else:\n",
        "            num_units = num_classes\n",
        "            activation = \"softmax\"\n",
        "        self.dense = layers.Dense(num_units, activation=activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "outputs = Classifier(num_classes=10)(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxEyJC6QRzg7"
      },
      "source": [
        "**Creating a subclassed model that includes a Functional model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5-4F5_k7Rzg8"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(64,))\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.dense = layers.Dense(64, activation=\"relu\")\n",
        "        self.classifier = binary_classifier\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features = self.dense(inputs)\n",
        "        return self.classifier(features)\n",
        "\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DfEmNk3Rzg8"
      },
      "source": [
        "### Remember: Use the right tool for the job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW8rPKx1Rzg8"
      },
      "source": [
        "## Using built-in training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAG3uJE1Rzg8"
      },
      "source": [
        "**The standard workflow: `compile()`, `fit()`, `evaluate()`, `predict()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LsAGtFbGRzg8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 55s 35ms/step - loss: 0.2983 - accuracy: 0.9120 - val_loss: 0.1512 - val_accuracy: 0.9568\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: 0.1698 - accuracy: 0.9530 - val_loss: 0.1154 - val_accuracy: 0.9679\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.1411 - accuracy: 0.9629 - val_loss: 0.1166 - val_accuracy: 0.9685\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.1046 - accuracy: 0.9727\n",
            "313/313 [==============================] - 2s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def get_mnist_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]\n",
        "\n",
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=3,\n",
        "          validation_data=(val_images, val_labels))\n",
        "test_metrics = model.evaluate(test_images, test_labels)\n",
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3QOu-0fRzg8"
      },
      "source": [
        "### Writing your own metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdwsYyGgRzg8"
      },
      "source": [
        "**Implementing a custom metric by subclassing the `Metric` class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dPOxgBXqRzg8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class RootMeanSquaredError(keras.metrics.Metric):\n",
        "\n",
        "    def __init__(self, name=\"rmse\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
        "        self.total_samples = self.add_weight(\n",
        "            name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n",
        "        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "        self.mse_sum.assign_add(mse)\n",
        "        num_samples = tf.shape(y_pred)[0]\n",
        "        self.total_samples.assign_add(num_samples)\n",
        "\n",
        "    def result(self):\n",
        "        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.mse_sum.assign(0.)\n",
        "        self.total_samples.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCJjAYFORzg8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.2954 - accuracy: 0.9131 - rmse: 7.1832 - val_loss: 0.1565 - val_accuracy: 0.9567 - val_rmse: 7.3631\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 57s 36ms/step - loss: 0.1651 - accuracy: 0.9539 - rmse: 7.3575 - val_loss: 0.1181 - val_accuracy: 0.9690 - val_rmse: 7.4065\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 57s 36ms/step - loss: 0.1350 - accuracy: 0.9635 - rmse: 7.3901 - val_loss: 0.1046 - val_accuracy: 0.9734 - val_rmse: 7.4227\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.1009 - accuracy: 0.9746 - rmse: 7.4362\n"
          ]
        }
      ],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\", RootMeanSquaredError()]) # you can check rmse\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=3,\n",
        "          validation_data=(val_images, val_labels))\n",
        "test_metrics = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z93FR9MeRzg8"
      },
      "source": [
        "### Using callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO_DiF7fRzg8"
      },
      "source": [
        "#### The EarlyStopping and ModelCheckpoint callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IB12AW2Rzg9"
      },
      "source": [
        "**Using the `callbacks` argument in the `fit()` method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UzP2E_qFRzg9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2903 - accuracy: 0.9149 - val_loss: 0.1548 - val_accuracy: 0.9560\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.1619 - accuracy: 0.9538 - val_loss: 0.1235 - val_accuracy: 0.9654\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.1387 - accuracy: 0.9631 - val_loss: 0.1232 - val_accuracy: 0.9688\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.1257 - accuracy: 0.9676 - val_loss: 0.1101 - val_accuracy: 0.9725\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.1156 - accuracy: 0.9715 - val_loss: 0.1052 - val_accuracy: 0.9753\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 53s 34ms/step - loss: 0.1069 - accuracy: 0.9734 - val_loss: 0.1070 - val_accuracy: 0.9768\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 57s 36ms/step - loss: 0.1032 - accuracy: 0.9753 - val_loss: 0.1130 - val_accuracy: 0.9770\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1017 - accuracy: 0.9768 - val_loss: 0.1149 - val_accuracy: 0.9768\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.0977 - accuracy: 0.9777 - val_loss: 0.1155 - val_accuracy: 0.9773\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 37s 23ms/step - loss: 0.0962 - accuracy: 0.9782 - val_loss: 0.1165 - val_accuracy: 0.9790\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2444b9ada90>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "callbacks_list = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=2,\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"checkpoint_path.keras\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "    )\n",
        "]\n",
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(val_images, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "j275l20DRzg9"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"checkpoint_path.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C7NzX5ERzg9"
      },
      "source": [
        "### Writing your own callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P-0TsMMRzg9"
      },
      "source": [
        "**Creating a custom callback by subclassing the `Callback` class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2XJMImKKRzg9"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs):\n",
        "        self.per_batch_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        plt.clf()\n",
        "        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n",
        "                 label=\"Training loss for each batch\")\n",
        "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plot_at_epoch_{epoch}\")\n",
        "        self.per_batch_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "bqw_EIdpRzg9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   4/1563 [..............................] - ETA: 30s - loss: 1.9714 - accuracy: 0.3281  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_train_batch_end` time: 0.0132s). Check your callbacks.\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2933 - accuracy: 0.9137 - val_loss: 0.1433 - val_accuracy: 0.9570\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.1660 - accuracy: 0.9530 - val_loss: 0.1203 - val_accuracy: 0.9673\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 62s 40ms/step - loss: 0.1391 - accuracy: 0.9636 - val_loss: 0.1158 - val_accuracy: 0.9715\n",
            "Epoch 4/10\n",
            "1341/1563 [========================>.....] - ETA: 8s - loss: 0.1260 - accuracy: 0.9681"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m get_mnist_model()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mLossHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:385\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    383\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 385\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:292\u001b[0m, in \u001b[0;36mCallbackList._process_logs\u001b[1;34m(self, logs, is_batch_hook)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batch_hook \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_hooks_support_tf_logs:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n\u001b[1;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
            "File \u001b[1;32mc:\\Users\\Jeongho Seo\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          callbacks=[LossHistory()],\n",
        "          validation_data=(val_images, val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI7zaQqWRzg9"
      },
      "source": [
        "### Monitoring and visualization with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh5lO5stRzg9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.8670 - loss: 0.4468 - val_accuracy: 0.9606 - val_loss: 0.1423\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9532 - loss: 0.1611 - val_accuracy: 0.9662 - val_loss: 0.1231\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9638 - loss: 0.1326 - val_accuracy: 0.9715 - val_loss: 0.1043\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9668 - loss: 0.1163 - val_accuracy: 0.9751 - val_loss: 0.0950\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9701 - loss: 0.1069 - val_accuracy: 0.9742 - val_loss: 0.1004\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9711 - loss: 0.1044 - val_accuracy: 0.9784 - val_loss: 0.0915\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9771 - loss: 0.0857 - val_accuracy: 0.9762 - val_loss: 0.0903\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.9787 - loss: 0.0754 - val_accuracy: 0.9790 - val_loss: 0.0939\n",
            "Epoch 9/10\n",
            "\u001b[1m1252/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9809 - loss: 0.0685"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[79], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[0;32m      7\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/full_path_to_your_log_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_images, train_labels,\n\u001b[0;32m     10\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     11\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(val_images, val_labels),\n\u001b[0;32m     12\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[tensorboard])\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m ):\n\u001b[0;32m    219\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gen_optional_ops\u001b[38;5;241m.\u001b[39moptional_has_value(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    178\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[1;34m(optional, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m    173\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptionalHasValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, optional)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "tensorboard = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/full_path_to_your_log_dir\",\n",
        ")\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          validation_data=(val_images, val_labels),\n",
        "          callbacks=[tensorboard])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-goO_WeRzg9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 12176), started 0:35:28 ago. (Use '!kill 12176' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-7070445a372abf36\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-7070445a372abf36\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /full_path_to_your_log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS56NSPFRzg-"
      },
      "source": [
        "## Writing your own training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTHaUrLqRzg-"
      },
      "source": [
        "### Training versus inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX80VcSJRzg-"
      },
      "source": [
        "### Low-level usage of metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gikkICBRzg-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result: 1.00\n"
          ]
        }
      ],
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result: {current_result:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdnnUP9ERzg-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of values: 2.00\n"
          ]
        }
      ],
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for value in values:\n",
        "    mean_tracker.update_state(value)\n",
        "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrlbW_XuRzg-"
      },
      "source": [
        "### A complete training and evaluation loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpCtPWRRzg-"
      },
      "source": [
        "**Writing a step-by-step training loop: the training step function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPZM-1aXRzg-"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "loss_tracking_metric = keras.metrics.Mean()\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"loss\"] = loss_tracking_metric.result()\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8rm0cHuRzg-"
      },
      "source": [
        "**Writing a step-by-step training loop: resetting the metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoPAm1uWRzg-"
      },
      "outputs": [],
      "source": [
        "def reset_metrics():\n",
        "    for metric in metrics:\n",
        "        metric.reset_state()\n",
        "    loss_tracking_metric.reset_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG2sUe-_Rzg-"
      },
      "source": [
        "**Writing a step-by-step training loop: the loop itself**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0w4VZUSRzg-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results at the end of epoch 0\n",
            "...sparse_categorical_accuracy: 0.9147\n",
            "...loss: 0.2877\n",
            "Results at the end of epoch 1\n",
            "...sparse_categorical_accuracy: 0.9543\n",
            "...loss: 0.1583\n",
            "Results at the end of epoch 2\n",
            "...sparse_categorical_accuracy: 0.9631\n",
            "...loss: 0.1299\n"
          ]
        }
      ],
      "source": [
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    reset_metrics()\n",
        "    for inputs_batch, targets_batch in training_dataset:\n",
        "        logs = train_step(inputs_batch, targets_batch)\n",
        "    print(f\"Results at the end of epoch {epoch}\")\n",
        "    for key, value in logs.items():\n",
        "        print(f\"...{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qAfvl4lRzg_"
      },
      "source": [
        "**Writing a step-by-step evaluation loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDL67U_tRzg_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9694\n",
            "...val_loss: 0.1122\n"
          ]
        }
      ],
      "source": [
        "def test_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[\"val_\" + metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "    return logs\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "    logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbNbaughRzg_"
      },
      "source": [
        "### Make it fast with tf.function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjNe3be4Rzg_"
      },
      "source": [
        "**Adding a `tf.function` decorator to our evaluation-step function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2MZSLHzRzg_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9694\n",
            "...val_loss: 0.1122\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def test_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[\"val_\" + metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "    return logs\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "    logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuDi1zi5Rzg_"
      },
      "source": [
        "### Leveraging fit() with a custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFydZo8BRzg_"
      },
      "source": [
        "**Implementing a custom training step to use with `fit()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYB4duDRzg_"
      },
      "outputs": [],
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = loss_fn(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mEEu5ndRzg_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - loss: 0.4419\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.1697\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.1318\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x20f9c5587a0>"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop())\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eTvVutuRzg_"
      },
      "outputs": [],
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.compiled_loss(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "        self.compiled_metrics.update_state(targets, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lYy-yeWRzhA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:667: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight, training)`.\n",
            "  warnings.warn(\n",
            "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:642: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
            "```\n",
            "for metric in self.metrics:\n",
            "    metric.update_state(y, y_pred)\n",
            "```\n",
            "\n",
            "  return self._compiled_metrics_update_state(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - sparse_categorical_accuracy: 0.8639 - loss: 0.1000\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - sparse_categorical_accuracy: 0.9520 - loss: 0.1000\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - sparse_categorical_accuracy: 0.9619 - loss: 0.1000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x20f9c2dcc50>"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58hXPP2SRzhA"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter07_working-with-keras.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
